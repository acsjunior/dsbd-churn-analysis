---
title: "Classificação de Churn Utilizando um Modelo de Regressão Logística"
subtitle: "Customer Churn Classification Using a Logistic Regression Model"
author: "Antonio C. da Silva Júnior"
email: "juniorssz@gmail.com"
affiliation: "Campus Santos, Universidade Paulista, Av. Conselheiro Nébias 766, Boqueirão, 11045-002, Santos, SP, Brasil"
# date: "8/24/2020"
output:
  bookdown::pdf_document2:
    template: rbef_template.tex
    keep_tex: true
    pandoc_args: ["--natbib"]
bibliography: referencias.bib
biblio-style: unsrt
lang: pt-BR
resumo: >
  O desenvolvimento de estratégias para retenção de clientes se tornou uma prática comum entre companhias de diversos segmentos, uma vez que relacionamentos de longo prazo com clientes estão associados à sobrevivência econômica e ao sucesso das empresas. Portanto, com o objetivo de antever clientes propensos a abandonar o relacionamento com uma startup brasileira, fenômeno conhecido como \textit{churn}, este artigo apresenta um modelo preditivo que possibilita a classificação de \textit{churn} e permite a interpretação dos motivos que impactam o desfecho. Após um extensivo processo de \textit{data wrangling}, aplicou-se a regressão logística fazendo uso de validação cruzada K-fold e do algortimo \textit{stepwise} para seleção de covariáveis. O modelo final, composto por 14 covariáveis, passou por uma análise de diagnóstico por meio dos resíduos quantilicos aleatorizados e teve o poder preditivo avaliado através da curva ROC, matriz de confusão e de métricas de avaliação. Em todas as etapas do estudo o modelo foi considerado adequado para o negócio e, além de exibir um bom poder preditivo, demonstrou-se capaz de fornecer \textit{insights} para ações de marketing personalizadas e otimizadas com foco na retenção dos clientes propensos a dar \textit{churn}.
palavras_chave: "Churn, Retenção de clientes, CRM, Regressão logística, Stepwise."
abstract: >
  Developing strategies for customer retention became a common practice among companies from different segments, since long-term relationships with customers are associated to the economic survival and success of companies. Therefore, with the goal of predicting the customer churn of a Brazilian startup, this article presents a predicitve model that classifies the customer churn and allows interpreting the reasons that impact the outcome. After an extensive data wrangling proccess, the logistic regression was applied using K-fold cross-validation and the stepwise algorithm for covariate selection. The final model, composed by 14 covariates, has undergone a diagnostic analysis through randomized quantile residuals and its prediction power was evatuated by the ROC curve, confusion matrix and evaluating metrics. The model was considered appropriate for the business in all stages of the study and not only proved to have a good prediction power but also demonstrated itself capable of providing insights for executing optimized and customized marketing actions focusing on the retention of customers likely to churn.
keywords: "Customer churn, Customer retention, CRM, Logistic regression, Stepwise"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
options(knitr.table.format = "latex", scipen = 999)

TABLE_FONT_SIZE <- 9
SEED <- 111

source("../../scripts/install.R")

theme_set(theme_bw())
```

# Introdução

A importância do relacionamento de longo prazo entre cliente e empresa é um assunto vastamente discutido na literatura. Devido aos efeitos do aprendizado e à redução dos custos de manutenção, atender um cliente se torna menos dispendioso a cada ano adicional de relacionamento \cite{Ganesh2000}. Por conta do aumento dos custos para atração de novos clientes em um mercado competitivo e a potencial redução dos custos associados aos relacionamentos de longo prazo, a retenção de clientes se torna essencial para a sobrevivência econômica e o sucesso das empresas do setor de serviços \cite{HennigThurau2004}. De acordo com Gallo \cite{Gallo2014}, dependendo do estudo e do segmento no qual a empresa está inserida, o custo para adquirir um novo cliente pode ser de cinco a vinte e cinco vezes superior ao da manutenção de um cliente já existente.

O desenvolvimento de estratégias para retenção de clientes se tornou uma prática comum entre companhias de diversos segmentos, e em consequência, antever os clientes propensos a abandonar o relacionamento com a empresa, fenômeno conhecido como \textit{churn}, se tornou um anseio constante. Em um momento de generalizados esforços na direção da cultura orientada a dados, os modelos preditivos para detecção de \textit{churn}, predominantemente utilizados por grandes companhias no setor de telecomunicações, se tornaram ferramentas populares nas empresas, independentemente da magnitude e da área de atuação.

A literatura comprova que a modelagem preditiva para detecção de churn é um tema bastante explorado e que possibilita inúmeras maneiras de desfecho: Botelho e Tostes \cite{Botelho2010} ajustaram um modelo de regressão logística para predizer a probabilidade de \textit{churn} em uma grande empresa de varejo; Vafeiadis et al. \cite{Vafeiadis2015} tiveram sucesso, entre os métodos comparados, na classificação de churn através do SVM (kernel polinomial) com AdaBoost em uma empresa de telecomunicações; Baseados nos dados de avaliações online de clientes, Kumar e Yadav \cite{Kumar2020} propuseram um modelo preditivo para detecção de churn baseado em regras, através de redes neurais artificiais e teoria dos conjuntos aproximados.

Com base nos dados disponibilizados pelo Olist, startup brasileira que tem como principal produto uma plataforma digital para conectar vendedores de diversos segmentos aos grandes marketplaces, a proposta deste artigo é apresentar um modelo preditivo que possibilite não só a classificação de vendedores propensos a abandonar o relacionamento com a empresa, mas que também permita a interpretação dos motivos que possivelmente estejam impactando o desfecho. Diante da variedade de técnicas disponíveis e das particularidades de cada modelo de negócio, a escolha do algoritmo adequado se torna uma etapa crucial do processo de modelagem. Portanto, tendo como referência a abordagem de Silva Júnior, Almeida e Santos \cite{Junior2020}, que utilizaram uma modelagem híbrida multicritério considerando múltiplos decisores para a escolha de um modelo preditivo de \textit{churn}, o algoritmo escolhido para desenvolver o classificador proposto foi a regressão logística.

# Materiais e métodos

## Estruturação do conjunto de dados

Os dados utilizados neste trabalho referem-se a clientes do Olist e foram disponibilizados anonimizados e com variáveis quantitativas padronizadas com média 0 e desvio padrão 1. Considerando que estes clientes contrataram uma plataforma digital que possibilita a venda de produtos nos principais marketplaces, neste trabalho eles serão chamados de vendedores. Devido às características da arquitetura do banco de dados e às particularidades do negócio da companhia, houve a necessidade de realizar um longo processo de \textit{data wrangling}. Este processo inicia-se por um diagnóstico preliminar dos dados, ou seja, se estão no formato adequado, se respondem as perguntas que motivaram a análise e o que é necessário para colocá-los no formato ideal. Em seguida avalia-se a ocorrência de dados faltantes, valores inconsistentes e duplicatas e, por fim, realiza-se um processo de limpeza e transformação, de modo a se obter um conjunto de dados adequado para o estudo \cite{Kandel2011}.

### Definição da variável resposta e covariáveis de desempenho

Inicialmente foram definidos como \textit{churn} ($Y=1$) os vendedores que estiveram inativos por 30 dias corridos desde a data da última atividade e permaneceram no mesmo estado em definitivo, considerando como atividade o acesso à plataforma digital ou a ocorrência de uma venda online. Em seguida, em função da data de corte estabelecida conforme a Tabela \@ref(tab:dataDeCorte), foram mantidos no conjunto de dados somente os vendedores com pelo menos 90 dias de histórico. O período de 90 dias, finalizado na data de corte, foi dividido igualmente em dois subperíodos, onde foram calculadas métricas como faturamento, ticket médio, quantidade de produtos publicados, quantidade de pedidos cancelados, número de dias em atividade e etc., em cada cada um dos subperíodos. Em seguida, através da equação  $V2 / (V1 + V2)$, foi calculado o desempenho do vendedor em função de diversas métricas, sendo $V1$ e $V2$ os valores calculados para cada subperíodo.

As métricas desempenho, dada a natureza da equação de origem, possuem o comportamento explicado pela Tabela \@ref(tab:metricas). Ao término desta etapa foi obtido um conjunto de dados composto pela variável resposta (\textit{churn}) e, como covariáveis, 9 métricas de desempenho, onde cada observação representa um vendedor.

```{r dataDeCorte}
tb_corte <- data.frame(Vendedor = c("Definido como churn", "Em atividade normal"),
                  Data_de_corte = c("Última atividade", "Realização da análise"))

names(tb_corte)[2] <- "Data de corte"

kable(tb_corte, booktabs = T, caption = "Definição da data de corte") %>%
  kable_styling(font_size = TABLE_FONT_SIZE)
```

```{r metricas}
tb_metricas <- data.frame(Valor = c("0,5", "> 0,5", "< 0,5"),
                     Desempenho= c("Mantido", "Aumentado", "Reduzido"))

kable(tb_metricas, booktabs = T, caption = "Interpretação das métricas de desempenho") %>%
  kable_styling(font_size = TABLE_FONT_SIZE)
```

### Adição de outras covariáveis

Foram adicionadas covariáveis qualitativas que representam o estágio do vendedor, o plano contratado e a região de origem, bem como covariáveis quantitativas como o faturamento total, total de produtos publicados, quantidade total de pedidos e etc., resultando em um conjunto de dados com 23 covariáveis. 

### Criação de covariáveis binárias

Dada a necessidade de analisar o comportamento da variável resposta em função de uma covariável qualitativa com $n$ categorias, deve-se criar $n-1$ covariáveis binárias (dummies), que assumem valores iguais a 0 ou 1, ficando por conta do pesquisador decidir qual das categorias será a referência (dummy = 0) \cite{Favero2017}. Portanto, as covariáveis qualitativas adicionadas foram transformadas em binárias, resultando em um conjunto de dados composto por 32 variáveis e 11.131 observações.

## Modelo de regressão logística

O objetivo da regressão logística é o estudo da probabilidade de ocorrência de um evento de interesse ($Y$), apresentado na forma dicotômica ($Y=1$ se o evento de interesse ocorrer; $Y=0$, caso contrário), em função de um vetor de covariáveis ($X_1, ..., X_n$). Sua definição ocorre através da Equação \@ref(eq:logito), onde $\beta_j$ ($j = 0,1,2,...,p$) representa os parâmetros a serem estimados, sendo $\beta_0$ o intercepto e os demais, parâmetros de cada covariável. E o subscrito $i$ representa cada observação da amostra ($i = 1, 2,...,n$) \cite{Favero2017}.
\begin{equation}
\ln \left ( \dfrac{\pi_i}{1-\pi_i} \right ) = \beta_0 + \beta_1 X_{i1} + ... +  \beta_p X_{ip}(\#eq:logito)
\end{equation}

A Equação \@ref(eq:logito) modela a log-chance de ocorrência do evento de interesse, portanto, para obter uma expressão para a probabilidade de ocorrência do evento é necessário isolar matematicamente $\pi_i$, resultando na Equação \@ref(eq:probabilidade).
\begin{equation}
\pi_i = \dfrac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + ... +  \beta_p X_{ip})}}(\#eq:probabilidade)
\end{equation}

A estimação dos parâmetros $\beta_j$ é realizada por máxima verossimilhança, método que consiste em encontrar os parâmetros que maximizam a função de verossimilhança representada através da Equação \@ref(eq:verossimilhanca).
\begin{equation}
L =  \prod_{i=1}^{n} \left[ \pi_i^{Y_i} (1-\pi_i)^{1-Y_i} \right](\#eq:verossimilhanca)
\end{equation}

Entretanto, matematicamente é mais conveniente trabalhar com o logaritmo da função de verossimilhança, conhecido como função de log-verossimilhança \cite{Favero2017,Botelho2010}, representado através da Equação \@ref(eq:logverossimilhanca).

\begin{equation}
\log L = \sum_{i=1}^{n} \left\{ \big[Y_i\ln(\pi_i)\big] + \big[(1-Y_i)\ln(1-\pi_i)\big] \right\}(\#eq:logverossimilhanca)
\end{equation}

## Ajuste do modelo e seleção de covariáveis

A comparação entre dois modelos de regressão logística pode ser realizada através do Critério de Informação de Akaike (AIC), definido por $-2\log L + 2p$, onde $\log L$ é a log-verossimilhança maximizada e $2p$ é o termo de penalização, sendo $p$ o número de parâmetros do modelo, devendo-se selecionar o modelo que apresentar o menor valor de AIC. Entretanto, avaliar todas a combinações possíveis pode ser computacionalmente invivável, mesmo para um número moderado de covariáveis. Portanto, para ajudar a encontrar o melhor modelo com o menor número de covariáveis possível foi utilizado o algoritmo \textit{stepwise} \cite{Taconeli2019}. Por padrão o \textit{stepwise} utiliza a minimização do AIC como critério para seleção das covariáveis, porém, neste estudo optou-se por alterar o múltiplo de penalização do AIC de 2 para 3,841459, que corresponde ao valor do quantil da distribuição do $\chi^2$ com 1 grau de liberdade e 5% de significância, permitindo assim que fosse considerado o p-valor = 0,05 como valor crítico para a seleção das covariáveis em cada iteração do algoritmo.

```{r include=FALSE}
df_train <- read.csv("../../Data/anonymous_train.csv")
df_test <- read.csv("../../Data/anonymous_test.csv")

# set.seed(SEED)
# ctrl <- trainControl(method = "cv", number = 5)
# full_model <- train(form = factor(y) ~ .,
#                      data = df_train,
#                      method = "glm",
#                      family = "binomial",
#                      trControl = ctrl)
# 
# step_model <- train(form = factor(y) ~ .,
#                     data = df_train,
#                     method = "glmStepAIC",
#                     direction = "both",
#                     k = qchisq(0.05, df = 1, lower.tail = F),
#                     family = "binomial",
#                     trControl = ctrl)
# 
# anova(step_model$finalModel, full_model$finalModel, test = "Chisq")
# qchisq(0.05, df = 17, lower.tail = F)

# 
# final_model <- step_model$finalModel
# saveRDS(final_model, file = "../../data/final_model.rds")
final_model <- readRDS("../../data/final_model.rds")

# The step() selects the model based on the AIC, not the p-value. It uses the argument k to set a threshold and determine if a variable should be include in the model or know. By default k=2, which set up a threshold for the p value to pchisq(2, 1, lower.tail = F), which is 0.1572992. That means only variables with p-value < 0.1572992 will be excluded from the model. To set the p-value threshold to 0.05, we need to adjust the k argument value to qchisq(0.05, 1, lower.tail = F), which is 3.8414588.

# https://mingchen0919.github.io/machine-learning-with-r/linear-regression.html
```

O conjunto de dados foi separado aleatoriamente em duas partes, garantindo a proporção aproximada de 47,3% de ocorrência de \textit{churn} ($Y=1$) em ambas as amostras. A amostra menor, com 25% dos dados, foi separada para a etapa de avaliação do poder preditivo do modelo, ao passo que a amostra maior foi utilizada para o ajuste de dois modelos por valição cruzada K-fold com 5 folds \cite{Kohavi1995}, a partir de todas as covariáveis disponíveis. Um deles, denominado \textit{modelo completo}, foi ajustado da maneira tradicional, ao passo que o segundo modelo, denominado \textit{modelo restrito}, teve o ajuste realizado através do algoritmo \textit{stepwise}.

# Resultados e discussões

## Teste da razão da verossimilhança

Por meio do teste da razão da verossimilhança (TRV), representado através da Equação \@ref(eq:trv), é possível verificar a qualidade do ajuste do \textit{modelo completo}, ajustado com $j$ covariáveis, em comparação com o \textit{modelo restrito}, ajustado com $j-k$ covariáveis, sendo $k$ o número de covariáveis removidas do ajuste. Quando a estatística do TRV é inferior ao valor da distribuição do ${\chi}^2$ com $k$ graus de liberdade e 5% de significância, não rejeita-se a hipótese nula, ou seja, constata-se que a remoção de $k$ covariáveis não afeta a qualidade do ajuste do modelo \cite{Favero2017}. 

\begin{equation}
TRV = -2(\log L_{\text{completo}} -\log L_{\text{restrito}})(\#eq:trv)
\end{equation}

Ao realizar o teste, foi constatado que a remoção das 17 covariáveis através do algoritmo \textit{stepwise} não alterou a qualidade do ajuste, uma vez que a estatística do teste foi inferior ao valor da distribuição do ${\chi}^2$ com 17 graus de liberdade e 5% de significância. Portanto, optou-se pelo \textit{modelo restrito} para a continuidade do estudo, uma vez que este possui complexidade inferior com relação ao \textit{modelo completo}, sem perda de qualidade. A Tabela \@ref(tab:covars) exibe as 14 covariáveis selecionadas para o modelo.

```{r covars, results='asis'}
dicionario <- read.csv("../../Data/reference_names.csv")
dicionario <- dicionario[c(-1,-3)]
names(dicionario) <- c("Covariável", "Descrição", "Suporte")
covariaveis <- names(coef(final_model))
covariaveis <- covariaveis[covariaveis != "(Intercept)"]
covariaveis <- dicionario[dicionario$Covariável %in% covariaveis, ]
row.names(covariaveis) <- 1:nrow(covariaveis)
covariaveis$Covariável <- str_to_upper(covariaveis$Covariável)

notacoes <- function(x) {
  if(x == "percentual") {
    out <- "$[0,1]$"
  } else if(x == "1 = sim; 0 = não") {
    out <- "$\\{0,1\\}$"
  } else if(x == "discreta") {
    out <- "$\\mathbb{N}$"
  } else {
    out <- "$\\mathbb{R}_+$"
  }
}

covariaveis$Suporte <- sapply(covariaveis$Suporte, notacoes)

covariaveis %>%
  kable(booktabs = T, caption = "Covariáveis utilizadas pelo modelo", table.env='table*', escape = F) %>%
  kable_styling(font_size = TABLE_FONT_SIZE) %>%
  pack_rows("Métricas de desempenho", 1,5) %>%
  pack_rows("Qualitativas", 6, 9) %>%
  pack_rows("Quantitativas", 10, 14)
```

## Análise de diagnóstico

Com a intenção de generalizar o método de análise dos resíduos da regressão linear para todos os modelos lineares generalizados, Dunn e Smyth \cite{Dunn1996} propuseram os resíduos quantílicos aleatorizados, definidos por $r_i = \phi^{-1}(u_i)$, onde $\phi^{-1}$ é a inversa da função de distribuição acumulada da normal padrão e $u_i = F(y_i;\mu_i,\phi)$, com distribuição uniforme entre 0 e 1, é calculado com base na distribuição acumulada do modelo proposto. Caso o modelo logístico esteja bem ajustado, espera-se que os resíduos quantílicos aleatorizados se apresentem normalmente distribuídos e com variância constante \cite{Taconeli2015}. A análise da qualidade do ajuste, através dos resíduos, foi realizada de forma gráfica. Ao comparar os resíduos com os valores ajustados (Figura \@ref(fig:resplot)) é possível observar que estes apresentam variabilidade aproximadamente constante e estão centrados predominantemente em 0, entre -2 e 2. No gráfico quantil-quantil \cite{Wilk1968} (Figura \@ref(fig:qqplot)) nota-se que os resíduos estão, de forma razoável, aderentes à distribuição normal. Portanto, nesta etapa foi concluído que o modelo não apresentou violação dos pressupostos.

```{r resplot, fig.cap='Gráficos dos resíduos versus valores ajustados', out.width="100%", fig.align='center'}
q_residuals <- qres.binom(final_model)
predictions <- predict(final_model)
df_diagnosis <- data.frame(q_residuals, predictions)

df_diagnosis %>%
  ggplot(aes(x = predictions, y = q_residuals)) +
  geom_point(colour = "blue", alpha = 0.3) +
  geom_smooth(method="loess", colour = "red") +
  labs(x = "Valores ajustados", y = "Resíduos") +
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14, face="bold"),
        title=element_text(size=16, face="bold"))
```

```{r qqplot, fig.cap='Gráfico quantil-quantil', out.width="100%", fig.align='center'}
df_diagnosis %>%
  ggplot(aes(sample = q_residuals)) +
  stat_qq(colour = "blue", alpha = 0.3, size = 3) +
  stat_qq_line(colour = "red") +
  labs(x = "Quantis teóricos", y = "Quantis amostrais") +
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14, face="bold"),
        title=element_text(size=16, face="bold"))
```

## Análise e interpretação das estimativas dos parâmetros

A Tabela \@ref(tab:coefs) apresenta as estimativas dos parâmetros do modelo para cada covariável utilizada. Através da estatística $z$ de Wald, definida pela Equação \@ref(eq:wald), onde $\hat{\beta}_j$ é a estimativa de um particular parâmetro $\beta_j$ do modelo e $ep(\hat{\beta}_j)$ é o seu erro padrão, é possível obter a significância estatística de cada estimativa. Calculadas as estatísticas $z$ de Wald, através da distribuição normal padrão a um determinado nível de significância obtemos os respectivos valores críticos e verificamos se estes rejeitam ou não a hipótese nula do teste $z$ de Wald ($H_0:\hat{\beta}_j=0$) \cite{Favero2017}. Em outras palavras, o p-valor do teste $z$ de Wald indica a probabilidade de $\beta$ ser tão ou mais extremo que $|z_{\hat{\beta}_j}|$.

\begin{equation}
z_{\hat{\beta}_j}=\dfrac{\hat{\beta}_j}{ep(\hat{\beta}_j)}(\#eq:wald)
\end{equation}

```{r pvalue}
# beta <- 0.95179
# ep <- 0.42032
# z <- beta/ep
# p_value <- 2*(1-pnorm(abs(z)))
```

Através da Tabela \@ref(tab:coefs) observa-se que todas as estimativas apresentaram significância ao nível de 5%, não sendo necessária qualquer intervenção adicional no modelo final obtido a partir do algoritmo \textit{stepwise}.

```{r coefs}
coeficientes <- summary(final_model)
coeficientes <- as.data.frame(coeficientes$coefficients)

names(coeficientes) <- c("Estimativa", "Erro padrão", "Wald", "P-valor")
coeficientes$Covariável <- row.names(coeficientes)
row.names(coeficientes) <- 1:nrow(coeficientes)

coeficientes <- coeficientes %>%
  dplyr::select(Covariável, everything())

coeficientes[c(2,3,4,5)] <- round(coeficientes[c(2,3,4,5)], 4)
coeficientes$Covariável <- str_to_upper(coeficientes$Covariável)
coeficientes$Covariável[1] <- "Intercepto"

coeficientes %>%
  kable(booktabs = T, caption = "Estimativas dos parâmetros do modelo") %>%
  kable_styling(font_size = TABLE_FONT_SIZE)
```

Através da Equação \@ref(eq:odds), obtida a partir da Equação \@ref(eq:logito), é possível modelar a chance de ocorrência do evento de interesse para uma particular observação e, em consequência, avaliar o quanto a chance de ocorrência do evento de interesse se altera em média, em função de uma particular estimativa. Adicionalmente podemos dizer que o aumento de $k$ unidades em uma particular covariável, mantidas as demais condições constantes, multiplica a chance de ocorrência do evento de interesse por $e^{k\hat{\beta}_j}$, onde $\hat{\beta}_j$ representa a estimativa do parâmetro desta particular covariável \cite{Favero2017}.

\begin{equation}
\dfrac{\pi_i}{1-\pi_i}= e^{\beta_0 + \beta_1 X_{i1} + ... +  \beta_p X_{ip}}(\#eq:odds)
\end{equation}

As Tabelas \@ref(tab:odds) e \@ref(tab:oddsmetricas) representam as estimativas dos parâmetros do modelo e estão ordenadas decrescentemente em função da variação absoluta da chance de ocorrência de \textit{churn}. Suas interpretações podem ser realizadas da seguinte forma, começando pela Tabela \@ref(tab:odds): a chance de \textit{churn} fica multiplicada por $e^{-1,6460}=0,1928$ para 1 unidade a mais na covariável X31, mantidas as demais condições constantes. Em outras palavras, o acréscimo de 1 unidade na covariável X31 impacta na redução da chance de \textit{churn} em 81%, fixadas as demais covariáveis. Como as covariáveis quantitativas foram padronizadas com média 0 e desvio padrão 1, é importante ressaltar que 1 unidade na covariável X31 não representa 1 dia de atividade, assim sendo, neste estudo a melhor forma de interpretar as estimativas dos parâmetros das covariáveis quantitativas é assimilando que estimativas menores que zero reduzem em média a chance de \textit{churn}, mediante ao aumento do valor de suas respectivas covariáveis, ao passo que a estimativa maior que zero aumenta em média a chance de \textit{churn}, à medida que o valor de sua respectiva covariável também aumenta, fixadas as demais covariáveis. Quanto às covariáveis qualitativas, também representadas na Tabela \@ref(tab:odds), podemos interpretá-las conforme o exemplo: a chance de \textit{churn} para os vendedores que estão no estágio I (X13) é 93% menor com relação aos vendedores que não estão no mesmo estágio, mantidas as demais condições constantes. Por fim, com relação às covariáveis de desempenho (Tabela \@ref(tab:oddsmetricas)), dadas as suas suas características representadas na Tabela \@ref(tab:metricas), suas interpretações podem ser realizadas de acordo com o exemplo: a chance de \textit{churn} de um vendedor que melhorou o seu desempenho nos subperíodos avaliados com relação ao faturamento ($X6>0,5$), é em média mais de 34% menor, fixadas as demais covariáveis. Em contrapartida, a chance de \textit{churn} de um vendedor que piorou o seu desempenho nos subperídos avaliados com relação ao faturamento ($X6<0,5$), é também menor em média, entretanto, em um valor percentual inferior a 34.

Nesta etapa foi constatado que o aumento do número de dias em atividade no perído (X31), estar tanto no estágio I (X13) como no estágio R (X14) e a melhora do desempenho quanto ao faturamento (X6), são os fatores que mais impactam a redução da chance de \textit{churn}.

```{r odds}
odds <- coeficientes[c("Covariável", "Estimativa")]
odds <- odds[odds$Covariável != "Intercepto",]

var_odds <- dicionario[dicionario$Suporte != "percentual",]
var_odds <- str_to_upper(var_odds$Covariável)
var_odds <- odds[odds$Covariável %in% var_odds,]
var_odds$Chance <- round(with(var_odds, exp(Estimativa)), 4)
var_odds$Variação <- round((var_odds$Chance - 1)*100, 0)

var_odds <- var_odds %>%
  arrange(desc(abs(Variação)))

kable(var_odds, booktabs = T, caption = "Chances de ocorrência de churn para covariáveis quantitativas e qualitativas",
      col.names = c("Covariável", "Estimativa", "Chance", "Variação (%)")) %>%
  kable_styling(font_size = TABLE_FONT_SIZE)
```

```{r oddsmetricas}
var_odds_metricas <- dicionario[dicionario$Suporte == "percentual",]
var_odds_metricas <- str_to_upper(var_odds_metricas$Covariável)
var_odds_metricas <- odds[odds$Covariável %in% var_odds_metricas,]
var_odds_metricas$Chance <- round(with(var_odds_metricas, exp(0.5 * Estimativa)), 4)
var_odds_metricas$Variação <- round((var_odds_metricas$Chance - 1)*100, 0)

var_odds_metricas <- var_odds_metricas %>%
  arrange(desc(abs(Variação)))

kable(var_odds_metricas, booktabs = T, caption = "Chances de ocorrência de churn para covariáveis de desempenho",
      col.names = c("Covariável", "Estimativa", "Chance (0,5)", "Variação (%)")) %>%
  kable_styling(font_size = TABLE_FONT_SIZE)
```

## Avaliação do poder preditivo do modelo

Para possibilitar a avaliação do poder preditivo do modelo na amostra de validação, é necessário antes definir o valor de \textit{cutoff}, ou seja, um ponto de corte de modo que as observações com probabilidade de ocorrência de \textit{churn} superior ao \textit{cutoff} sejam classificadas como \textit{churn} ($Y=1$) e, caso contrário, classificadas como não \textit{churn} ($Y=0$). A escolha do \textit{cutoff} foi realizada através da análise das curvas de sensibilidade e especificidade em função dos valores de \textit{cutoff}, e da curva ROC (\textit{Receiver Operating Characteristic}), gráfico que apresenta a variação da sensibilidade em função de (1 - especificidade), que mostra o comportamento do \textit{trade off} entre a sensibilidade e a especificidade em função da alteração do \textit{cutoff} \cite{Favero2017}. Atrávés do cálculo da área sob a curva ROC (AUC - \textit{Area Under the Curve}) é possível avaliar a eficiênca global do modelo, sendo $AUC=1$ o melhor valor possível. O modelo em estudo apresentou $AUC = 0,8894$, o que indica uma boa eficiência global. Analisadas as curvas de sensibilidade e especificidade (Figura \@ref(fig:senscurve)) e a curva ROC (Figura \@ref(fig:roc)), e considerando os requisitos do negócio, optou-se por um valor de \textit{cutoff} que garantisse o equilíbrio entre sensibilidade e especificidade. Portanto, para continuidade do estudo foram consideradas como \textit{churn} (Y=1) as observações com probabilidade de ocorrência de \textit{churn} superior a 0,55.

```{r senscurve, fig.cap='Curvas de sensibilidade e especificidade', out.width="100%", fig.align='center'}
pred <- predict(final_model, newdata = df_test, type = "response")
rocobj <- roc(df_test$y, pred, plot=F, ci=T, ci.sp = T)
prevalence <- prop.table(table(rbind(df_train, df_test)$y))[2]
# auc(rocobj)

# cutoff <- as.numeric(coords(rocobj, x = "best", best.method = "closest.topleft",
#                             best.weights=c(1, prevalence), transpose = F)[1])
# coords(rocobj, x = cutoff, ret = c("sensitivity", "specificity", "accuracy"), transpose = F)

# cutoff <- as.numeric(coords(rocobj, x = "best", best.method = "closest.topleft", best.weights=c(2, prevalence), transpose = F)[1])

cutoff <- 0.55
# coords(rocobj, x = cutoff, ret = c("sensitivity", "specificity", "accuracy", "youden", "closest.topleft"), transpose = F)

df_sens <- data.frame(Sensibilidade = rocobj$sensitivities,
                      Especificidade = rocobj$specificities,
                      Cutoff = rocobj$thresholds)
df_sens$Cutoff[1] <- 0
df_sens$Cutoff[nrow(df_sens)] <- 1

pred <- as.factor(ifelse(pred > cutoff, 1, 0))
cm <- table(pred, df_test$y)
VN <- cm[1,1]
VP <- cm[2,2]
FN <- cm[1,2]
FP <- cm[2,1]

sens <- VP / (VP + FN)
spec <- VN / (VN + FP)
accu <- (VN + VP) / (VN + VP + FN + FP)

df_sens <- df_sens %>%
  pivot_longer(cols = c("Sensibilidade", "Especificidade"),
               names_to = "var",
               values_to = "value")

df_sens$var <- as.factor(df_sens$var)
df_sens$var <- factor(df_sens$var, levels = rev(levels(df_sens$var)))

ggplot(df_sens, aes(x = Cutoff, y = value, colour = var)) +
  geom_line(size = 1) +
  labs(x = "Cutoff", y = "", colour = "") +
  scale_colour_manual(values = c("blue", "red")) +
  annotate("point", x = cutoff, y = accu, colour = "black", size = 4, alpha = 0.8) +
  annotate(geom = "text", x = cutoff+0.07, y = accu, label = "Cutoff", color = "black") +
  # scale_x_continuous(breaks = c(0, 0.2, 0.4, cutoff, 0.8, 1)) +
  # geom_vline(xintercept = cutoff, linetype = "dotted", size = 1, colour = "gray") +
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14, face="bold"),
        title=element_text(size=16, face="bold"),
        legend.text=element_text(size=12),
        legend.position = "top",
        legend.direction = "horizontal")
```

```{r roc, fig.cap='Curva ROC', out.width="100%", fig.align='center'}
ggroc(rocobj, colour = "blue", size = 1) +
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), size = .1, colour = "gray") +
  labs(x = "1 - Especificidade", y = "Sensibilidade") +
  annotate("point", x = spec, y = sens, colour = "black", size = 4, alpha = 0.8) +
  annotate(geom = "text", x = spec-0.06, y = sens, label = "Cutoff", color = "black") +
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14, face="bold"),
        title=element_text(size=16, face="bold"))
```

Definido o valor de \textit{cutoff}, através do cruzamento dos valores preditos pelo modelo e os valores observados, foi construída a matriz de confusão (Tabela \@ref(tab:cmatrix)), que apresenta em sua diagonal principal o número de classificações corretas e, na diagonal secundária, o número de classificações incorretas. A partir da matriz de confusão foram calculadas as seguintes métricas de avaliação: sensibilidade (taxa de classificação correta entre as observações com a ocorrência de \textit{churn}), especificidade (taxa de classificação correta entre as observações sem a ocorrência de \textit{churn}) e acurácia (taxa global de classificações corretas), representadas pelas equações \@ref(eq:sensitivity), \@ref(eq:specificity) e \@ref(eq:accuracy), respectivamente. 

```{r, cmatrix}
# confusionMatrix(pred, as.factor(df_test$y), positive = "1")

# cmatrix <- data.frame(Predito = c("0", "1"),
#                       `0` = c(VN, FP),
#                       `1` = c(FN, VP))

cmatrix <- data.frame(Predito = c("0", "1"),
                      `0` = c("Verdadeiro negativo (VN)", "Falso positivo (FP)"),
                      `1` = c("Falso negativo (FN)", "Verdadeiro positivo (VP)"))

names(cmatrix) <- c("Predito", "0", "1")

cmatrix %>%
  kable(booktabs = T, caption = "Matriz de confusão") %>%
  add_header_above(c("", "Observado"=2)) %>%
  kable_styling(font_size = TABLE_FONT_SIZE)
```

\begin{equation}
S = \dfrac{VP}{VP+FN}(\#eq:sensitivity)
\end{equation}

\begin{equation}
E = \dfrac{VN}{VN+FP}(\#eq:specificity)
\end{equation}

\begin{equation}
A = \dfrac{VN+VP}{VN+VP+FN+FP}(\#eq:accuracy)
\end{equation}

Com a sensibilidade de 0,8164, especificidade de 0,8111 e acurácia de 0,8136 (com invervalo de confiança de 0,7986 a 0,8279), o poder preditivo do modelo foi considerado adequado para o negócio.

# Conclusões

Através deste trabalho pretendeu-se construir um modelo para classificação de \textit{churn} interpretável e com poder preditivo adequado para o negócio. Entre as diversas técnicas disponíveis, a regressão logística foi escolhida por ser altamente confiável, por possibilitar a interpretação direta das estimativas dos parâmetros e por oferecer uma resposta na escala de probabilidade, permitindo aos decisores não só identificar os vendedores propensos a abandonar o relacionamento com a empresa, mas também ordená-los em função da probabilidade da ocorrência do \textit{churn}.

Por meio dos resultados apresentados, pôde-se constatar que o modelo proposto é capaz de atender a necessidade do negócio, uma vez que além do bom poder preditivo apresentado, oferece insights para ações de marketing personalizadas e otimizadas com foco na retenção dos vendedores propensos dar \textit{churn}.

Por fim, a abordagem utilizada na definição da variável resposta e na criação de métricas para avaliação de desempenho demonstrou-se eficaz no processo de modelagem, portanto, espera-se que este trabalho seja capaz de apoiar estudos futuros em condições semelhantes.

# Agradecimentos

Ao Olist por prover todas as condições necessárias para o desenvolvimento do trabalho. Ao corpo docente da Especialização em \textit{Data Science} e \textit{Big Data} da Universidade Federal do Paraná - UFPR, pela qualidade do ensino oferecido, em especial ao  Prof. Walmes Zeviani pela orientação, inspiração e disponibilidade, inclusive nos finais de semana, e ao Prof. Cesar Taconeli, que através das suas excelentes aulas me inspirou o interesse por Modelos Lineares Generalizados. Ao Prof. Marcos Santos do Instituto Militar de Engenharia - IME, pelo suporte e incentivo à pesquisa.