---
title: "Utilizando a Regressão Logística para Classificação de Churn em Ambiente de Startup"
subtitle: "Using Logistic Regression for Customer Churn Classification in Startup Environment "
author: "Antonio C. da Silva Júnior"
email: "juniorssz@gmail.com"
affiliation: "Campus Santos, Universidade Paulista Av. Conselheiro Nébias 766, Boqueirão, 11045-002, Santos, SP, Brasil"
# date: "8/24/2020"
output:
  bookdown::pdf_document2:
    template: rbef_template.tex
    keep_tex: true
    pandoc_args: ["--natbib"]
bibliography: ../references.bib
biblio-style: unsrt
lang: pt-BR
resumo: "asdfasdf."
palavras_chave: "asdfasdf, asdfasdf, asdfasdf, asdfasdf, asdfasdf"
abstract: "asdfasdf."
keywords: "asdfasdf, asdfasdf, asdfasdf, asdfasdf, asdfasdf"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
options(knitr.table.format = "latex", scipen = 999)

TABLE_FONT_SIZE <- 10
SEED <- 111

source("../../scripts/install.R")

theme_set(theme_bw())
```

# Introdução

A importância do relacionamento de longo prazo entre cliente e empresa é um assunto vastamente discutido na literatura. Devido aos efeitos do aprendizado e à redução dos custos de manutenção, atender um cliente se torna menos dispendioso a cada ano adicional de relacionamento \cite{Ganesh2000}. Por conta do aumento dos custos para atração de novos clientes em um mercado competitivo e à potencial redução dos custos associados aos relacionamentos de longo prazo, a retenção de clientes se torna essencial para a sobrevivência e o sucesso econômico das empresas do setor de serviços \cite{HennigThurau2004}. De acordo com \cite{Gallo2014}, dependendo do estudo e do segmento no qual a empresa está inserida, o custo para adquirir um novo cliente pode ser de cinco a vinte e cinco vezes superior ao da manutenção de um cliente já existente.

O desenvolvimento de estratégias para retenção de clientes se tornou uma prática comum entre empresas de diversos segmentos, e em consequência, antever o abandono de clientes se tornou um anseio constante. Em um momento de generalizada de esforços na direção da cultura orientada a dados, os modelos preditivos para detecção de abandono de clientes, predominantemente utilizados por grandes companhias no setor de telecomunicações, se tornaram ferramentas populares nas empresas, independentemente da magnitude e da área de atuação.

A literatura comprova que a modelagem preditiva de abandono de clientes, também conhecida como modelagem preditiva de churn, é um tema bastante explorado e que possibilita inúmeras maneiras de desfecho: Botelho e Tostes \cite{Botelho2010} ajustaram um modelo de regressão logística para predizer a probabilidade de churn em uma grande empresa de varejo; Vafeiadis et al. \cite{Vafeiadis2015} tiveram sucesso, entre os métodos comparados, na classificação de churn através do SVM (kernel polinomial) com AdaBoost em uma empresa de telecomunicações; Baseados nos dados de avaliações online de clientes, Kumar e Yadav \cite{Kumar2020} propuseram um modelo preditivo de churn baseado em regras através de redes neurais artificiais e teoria dos conjuntos aproximados.

Com base nos dados de uma startup brasileira que tem como principal produto uma plataforma digital para conectar vendedores de diversos segmentos aos grandes marketplaces, a proposta deste artigo é apresentar um modelo preditivo que possibilite não só a classificação de vendedores propensos a abandonar a empresa, mas que também permita a interpretação dos motivos que possivelmente estejam impactando a predição. Diante da variedade de técnicas disponíveis e das particularidades de cada modelo de negócio, a escolha do algoritmo adequado se torna uma etapa crucial do processo de modelagem. Portanto, tendo como referência a abordagem de \cite{Junior2020}, que utilizaram uma modelagem híbrida multicritério considerando múltiplos decisores para a escolha de um modelo preditivo de churn, o algoritmo escolhido para desenvolver o classificador proposto foi a regressão logística.

# Materiais e métodos

## Estruturação do conjunto de dados

Os dados utilizados neste trabalho referem-se a clientes de uma startup paranaense, anonimizados e com variáveis quantitativas padronizadas com média 0 e desvio padrão 1. Considerando que estes clientes contrataram uma plataforma digital que possibilita a venda de produtos nos principais marketplaces, neste trabalho eles serão chamados de vendedores. Devido às características da arquitetura do banco de dados e às particularidades do negócio da companhia, houve a necessidade de realizar um longo processo de data wrangling. Este processo inicia-se por um diagnóstico preliminar dos dados, ou seja, se estão no formato adequado, se respondem as perguntas que motivaram a análise e o que é necessário para colocá-los no formato ideal. Em seguida avalia-se a ocorrência de dados faltantes, valores inconsistentes e duplicatas e, por fim, realiza-se um processo de limpeza e transformação, de modo a se obter um conjunto de dados adequado para o estudo \cite{Kandel2011}.

### Definição da resposta e covariáveis de desempenho

Inicialmente foram definidos como churn ($Y=1$) os vendedores que estiveram inativos por 30 dias corridos desde a data da última atividade e permaneceram no mesmo estado em definitivo, considerando como atividade o acesso à plataforma digital ou a ocorrência de uma venda online. Em seguida, em função da data de corte estabelecida conforme a Tabela \@ref(tab:dataDeCorte), foram mantidos no conjunto de dados somente os vendedores com pelo menos 90 dias de histórico. O período de 90 dias, finalizado na data de corte, foi dividido igualmente em dois subperíodos, onde foram calculadas métricas como faturamento, ticket médio, quantidade de produtos publicados, quantidade de pedidos cancelados, número de dias em atividade e etc., em cada cada um dos subperíodos. Em seguida, através da Equação \@ref(eq:desempenho), foi calculado desempenho do vendedor em função de diversas métricas, onde $V1$ e $V2$ são os valores calculados para cada subperíodo.

As métricas desempenho, data à natureza da equação de origem, possuem o comportamento explicado pela Tabela \@ref(tab:metricas). Ao término desta etapa foi obtido um conjunto de dados composto pela variável resposta (churn) e, como covariáveis, 10 métricas de desempenho, onde cada observação representa um vendedor.

```{r dataDeCorte}
tabela <- data.frame(Vendedor = c("Definido como churn", "Em atividade normal"),
                  Data_de_corte = c("Última atividade", "Realização da análise"))

names(tabela)[2] <- "Data de corte"

kable(tabela, booktabs = T, caption = "Definição da data de corte") %>%
  kable_styling(font_size = TABLE_FONT_SIZE)
```

\begin{equation}
Desempenho = \dfrac{V2}{V1+V2} (\#eq:desempenho)
\end{equation}

```{r metricas}
tabela <- data.frame(Valor = c("0,5", "> 0,5", "< 0,5"),
                     Desempenho= c("Mantido", "Aumentado", "Reduzido"))

kable(tabela, booktabs = T, caption = "Interpretação das métricas de desempenho") %>%
  kable_styling(font_size = TABLE_FONT_SIZE)
```

### Adição de outras covariáveis

Foram adicionadas covariáveis qualitativas que representam o estágio do vendedor, o plano contratado e a região de origem, bem como covariáveis quantitativas como o faturamento total, total de produtos publicados, quantidade total de pedidos e etc., resultando em um conjunto de dados com 26 covariáveis. 

### Criação de covariáveis binárias

Dada a necessidade de analisar o comportamento da variável resposta em função de uma covariável qualitativa com $n$ categorias, deve-se criar $n-1$ covariáveis binárias (dummies), que assumem valores iguais a 0 ou 1, ficando por conta do pesquisador decidir qual das categorias será a referência (dummy = 0) \cite{Favero2017}. Portanto, as covariáveis qualitativas adicionadas foram transformadas em binárias, resultando em um conjunto de dados composto por 35 variáveis e 11.131 observações.

## Modelos de regressão logística binária

O objetivo da regressão logística binária é o estudo da probabilidade de ocorrência de um evento de interesse ($Y$), apresentado na forma dicotômica ($Y=1$ se o evento de interesse ocorrer; $Y=0$, caso contrário), em função de um vetor vetor de covariáveis ($X_1, ..., X_n$). Sua definição ocorre através da Equação \@ref(eq:logito), onde $\beta_j$ ($j = 0,1,2,...,p$) representam os parâmetros a serem estimados, sendo $\beta_0$ o intercepto e os demais, parâmetros de cada covariável, e o subscrito $i$ representa cada observação da amostra ($i = 1, 2,...,n$) \cite{Favero2017}.
\begin{equation}
ln \left ( \dfrac{\pi_i}{1-\pi_i} \right ) = \beta_0 + \beta_1 X_{i1} + ... +  \beta_p X_{ip}(\#eq:logito)
\end{equation}

A Equação \@ref(eq:logito), conhecida como logito, modela a log-chance de ocorrência do evento de interesse. Portanto, para obter uma expressão para a probabilidade de ocorrência do evento é necessário isolar matematicamente $\pi_i$, resultando na Equação \@ref(eq:probabilidade).
\begin{equation}
\pi_i = \dfrac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + ... +  \beta_p X_{ip})}}(\#eq:probabilidade)
\end{equation}

A estimação dos parâmetros $\beta_i$ é realizada por máxima verossimilhança, método que consiste em encontrar, através da programação linear, os parâmetros que maximizam a função de verossimilhança representada através da Equação \@ref(eq:verossimilhanca).
\begin{equation}
L =  \prod_{i=1}^{n} \left[ \pi^{Y_i} (1-\pi_i)^{1-Y_i} \right](\#eq:verossimilhanca)
\end{equation}

Entretanto, matematicamente é mais conveniente trabalhar com o logaritmo da função de verossimilhança, conhecido como função de log-verossimilhança \cite{Favero2017,Botelho2010}, representado através da Equação \@ref(eq:logverossimilhanca).

\begin{equation}
logL = \sum_{i=1}^{n} \left\{ \big[Y_iln(\pi_i)\big] + \big[(1-Y_i)ln(1-\pi_i)\big] \right\}(\#eq:logverossimilhanca)
\end{equation}

## Seleção de covariáveis através do algoritmo stepwise

A comparação entre dois modelos pode ser realizada através do Critério de Informação de Akaike (AIC), definido por $-2logL + 2p$, onde $logL$ é a log-verossimilhança maximizada e $p$ é o número de parâmetros do modelo, devendo-se selecionar o modelo que apresentar o menor valor de AIC. Entretanto, avaliar todas a regressões possíveis, mesmo para um número moderado de covariáveis, pode ser computacionalmente invivável, mas como alternativa é possível utilizar o algoritmo \textit{stepwise}, que funciona da seguinte forma \cite{Taconeli2019}:

1. Ajuste do modelo com todas as covariáveis;

2. Avaliação tanto a exclusão como a inclusão de cada covariável, através do cálculo do AIC;

3. Exclusão (ou inclusão) da covariável cuja exclusão (ou inclusão) resulta em menor AIC;

4. Repete-se os passos 1 a 3 para o novo modelo e o processo continua até que nenhuma exclusão (ou inclusão) que resulte em menor AIC.

## Validação cruzada por K-fold

A validação cruzada por k-fold é uma técnica de reamostragem aplicada com o propósito de reduzir o vício e a variância do modelo, que funciona da seguinte forma \cite{Kohavi1995}:

1. Separação aleatória dos dados de treino em $k$ partições de tamanho aproximadamente igual (folds);

2. Isolamento de uma das partições e treino do modelo com os dados das demais partições concatenadas;

3. Validação do modelo com os dados da partição isolada, através de determinada métrica de avaliação;

4. Repete-se os passos 2 e 3 até que o modelo seja validado em todas as $k$ partições;

5. A métricas de avaliação de cada iteração são resumidas, normalmente através da média aritmética.

## Ajuste do modelo de regressão logística binária

```{r include=FALSE}
df_train <- read.csv("../../Data/anonymous_train.csv")
df_test <- read.csv("../../Data/anonymous_test.csv")

# set.seed(SEED)
# full_model <- glm(factor(y) ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 +
#                     x11 + x12 + x13 + x14 + x15 + x16 + x17 + x18 + x19 + x20 + x21 + x22 +
#                     x23 + x25 + x26 + x27 + x28 + x29 + x30 + x31 + x32 + x33 + x34,
#                   data = df_train,
#                   family = binomial())
# 
# step_model <- step(full_model, k = qchisq(0.05, df = 1, lower.tail = F), direction = "both")

# anova(step_model, full_model, test = "Chisq")
# qchisq(0.05, df = 14, lower.tail = F)
# 
# step_model_without_x13 <- update(step_model, ~ . -x13)
# anova(step_model_without_x13, step_model, test = "Chisq")
# qchisq(0.05, df = 1, lower.tail = F)

# final_model <- step_model
# saveRDS(final_model, file = "../../data/final_model.rds")
final_model <- readRDS("../../data/final_model.rds")
```

```{r covars}
dicionario <- read.csv("../../Data/reference_names.csv")
dicionario <- dicionario[c(-1,-3)]
names(dicionario) <- c("Variável", "Descrição", "Medição")
covariaveis <- names(coef(final_model))
covariaveis <- covariaveis[covariaveis != "(Intercept)"]
covariaveis <- dicionario[dicionario$Variável %in% covariaveis, ]
row.names(covariaveis) <- 1:nrow(covariaveis)

covariaveis %>%
  kable(booktabs = T, caption = "Covariáveis utilizadas pelo modelo", table.env='table*') %>%
  kable_styling(font_size = TABLE_FONT_SIZE) %>%
  pack_rows("Métricas de desempenho", 1,7) %>%
  pack_rows("Qualitativas", 8, 13) %>%
  pack_rows("Quantitativas", 14, 19)
```

O conjunto de dados foi separado aleatoriamente em duas partes, garantindo a proporção aproximada de 47,3% de ocorrência de churn ($Y=1$) em ambas as amostras. A amostra maior, com 75% dos dados, foi utilizada para o ajuste de dois modelos à partir de todas as covariáveis disponíveis. Um deles, denominado \textit{modelo completo}, foi ajustado da maneira tradicional, ao passo que o segundo modelo, denominado \textit{modelo restrito}, teve o ajuste realizado através do algoritmo \textit{stepwise}.

Através do teste da razão da verossimilhança, representado através da Equação \@ref(eq:trv), é possível verificar a qualidade do ajuste do \textit{modelo completo}, ajustado com $j$ covariáveis, em comparação com o \textit{modelo restrito}, ajustado com $j-k$ covariáveis, sendo $k$ o número de covariáveis removidas do ajuste \cite{Favero2017}. 

\begin{equation}
TRV = -2(logL_{completo} - logL_{restrito})(\#eq:trv)
\end{equation}

Quando a estatística do $TRV$ é inferior ao valor da distribuição do ${\chi}^2$ com $k$ graus de liberdade e 5% de significância, não rejeitamos a hipótese nula, ou seja, concluimos que a remoção de $k$ covariáveis não afeta a qualidade do ajuste do modelo. Ao realizar o teste, foi constatado que a remoção das 14 covariáveis, no \textit{modelo restrito}, não alterou a qualidade do ajuste, uma vez que a estatística do teste foi inferior ao valor da distribuição do ${\chi}^2$ com 14 graus de liberdade e 5% de significância. Portanto, optou-se pelo \textit{modelo restrito} para a continuidade do estudo, uma vez que este possui complexidade inferior com relação ao \textit{modelo completo}, sem perda de qualidade. A Tabela \@ref(tab:covars) exibe as 19 covariáveis selecionadas para o modelo.

```{r resplot, fig.cap='Gráficos dos resíduos versus valores ajustados', out.width="100%", fig.align='center'}
q_residuals <- qres.binom(final_model)
predictions <- predict(final_model)
df_diagnosis <- data.frame(q_residuals, predictions)

df_diagnosis %>%
  ggplot(aes(x = predictions, y = q_residuals)) +
  geom_point(colour = "blue", alpha = 0.3) +
  geom_smooth(method="loess", colour = "red") +
  labs(x = "Valores ajustados", y = "Resíduos") +
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14, face="bold"),
        title=element_text(size=16, face="bold"))
# plot(q_residuals ~ predictions, col = 'blue', xlab = 'Valores ajustados', ylab = 'Resíduos')
# lines(lowess(q_residuals ~ predictions), col = 'red', lwd = 2)
```

```{r qqplot, fig.cap='Gráfico quantil-quantil', out.width="100%", fig.align='center'}
# qqnorm(q_residuals, col = 'blue', xlab = 'Quantis teóricos', ylab = 'Quantís amostrais', main = "")
# qqline(q_residuals, lty = 2)
df_diagnosis %>%
  ggplot(aes(sample = q_residuals)) +
  stat_qq(colour = "blue", alpha = 0.3, size = 3) +
  stat_qq_line(colour = "red") +
  labs(x = "Quantis teóricos", y = "Quantis amostrais") +
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14, face="bold"),
        title=element_text(size=16, face="bold"))
```

Com a intenção de generalizar o método de análise dos resíduos da regressão linear para todos os modelos lineares generalizados, Dunn e Smyth \cite{Dunn1996} propuseram os resíduos quantílicos aleatorizados, definidos por $r_i = \phi^{-1}(u_i)$, onde $\phi^{-1}$ é a inversa da função de distribuição acumulada da normal padrão e $u_i = F(y_i;\mu_i,\phi)$, com distribuição uniforme entre 0 e 1, é calculado com base na distribuição acumulada do modelo proposto. Caso o modelo logístico esteja bem ajustado, espera-se resíduos quantílicos aleatorizados se apresentem normalmente distribuídos e com variância constante \cite{Taconeli2015}. 

A análise da qualidade do ajuste, através dos resíduos, foi realizada de forma gráfica. Ao comparar os resíduos com os valores ajustados (Figura \@ref(fig:resplot)) é possível observar que estes apresentam variabilidade aproximadamente constante e estão centrados predominantemente em 0, entre -2 e 2. No gráfico quantil-quantil \cite{Wilk1968} (Figura \@ref(fig:qqplot)) nota-se que os resíduos estão, de forma razoável, aderentes à distribuição normal. Portanto, pode-se considerar que o modelo está bem ajustado.

### Análise e interpretação dos coeficientes

```{r dicionario}
# dicionario <- read.csv("../../Data/reference_names.csv")
# dicionario <- dicionario[c(-1,-3)]
# names(dicionario) <- c("Variável", "Descrição", "Medição")

# kable(dicionario, booktabs = T, caption = "Dicionário do conjunto de dados estruturado", table.env='table*') %>%
#   kable_styling(font_size = TABLE_FONT_SIZE) %>%
#   pack_rows("Resposta", 1,1) %>%
#   pack_rows("Métricas de desempenho", 2,12) %>%
#   pack_rows("Categóricas", 13, 23) %>%
#   pack_rows("Numéricas", 24, 35)
```

```{r coefs}
# coeficientes <- summary(step_model)
# coeficientes <- as.data.frame(coeficientes$coefficients)
# 
# names(coeficientes) <- c("Coeficiente", "Erro padrão", "Wald", "P-valor")
# coeficientes$Variável <- row.names(coeficientes)
# row.names(coeficientes) <- 1:nrow(coeficientes)
# 
# coeficientes <- coeficientes %>%
#   dplyr::select(Variável, everything())
# 
# coeficientes[c(2,3,4,5)] <- round(coeficientes[c(2,3,4,5)], 4)
# 
# coeficientes$Variável[1] <- "Intercepto"
# 
# coeficientes %>%
#   kable(booktabs = T, caption = "Coeficientes das covariáveis utilizadas pelo modelo", table.env='table*') %>%
#   kable_styling(font_size = TABLE_FONT_SIZE)
```

asdfasdf

### Avaliação do poder preditivo do modelo

asdfasdf

# Conclusões

asdfasdf